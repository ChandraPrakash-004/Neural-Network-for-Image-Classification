Develop a neural network based classification network from scratch: This programming assignment uses MNIST digit dataset. It consists of large collection of handwritten digits from 0 to 9. These images are formated as 28x28 pixel gray scale images. The objective of this programming assignment is to design a neural network architecture that takes input as 28x28 image (or 784 dimensional vector) as input and predicts the digit information in it. Although there are diffrent varieties of neural network architecture to solve this task, this programming assignment uses only the feed forward network.
1. Load MNIST data and create train, test splits
    - The MNIST dataset consists of around 70,000 images. Divide the dataset into two segments: training and testing. Allocate 60,000 images for training and 10,000 images for testing 
    - Code for downloading the data and creating train-test splits is provided 
2. Design a simple classification network
    - Let us use three layer feed-forward neral network. Use 512 nodes in the hidden layers and 10 nodes in the output layer. The output ğ²
from the input ğ±
    is computed as follows 
    ğ²=â„(ğ–3ğ‘”(ğ–2ğ‘”(ğ–1ğ±)))
    where ğ–1âˆˆîˆ¾512Ã—768
,ğ–2âˆˆîˆ¾512Ã—512
,ğ–3âˆˆîˆ¾10Ã—512
    are the parameters of the network. g(.) is the hidden layer activation function. h(.) is the output layer activation function 
    - Consider g(.) as ReLU activation function. Softmax activation function should be used at the last layer h(.), to get the posterior probability of the classes. 
- Training classification network:
    - Flatten the 28x28 images to arrive at 784 dimensional vector. 
    - Randomly initialize the parameters of network, ğ–1âˆˆîˆ¾768Ã—512
,ğ–2âˆˆîˆ¾512Ã—512
,ğ–3âˆˆîˆ¾512Ã—10
    - Feedforward the batch of input vectors to get the posterior probability of classes. 
    - Compute the loss between the estimated posterior probabilities and the true targets. 
    - Update the parameters of network to minimize the loss function. 
        - Backpropagate the loss function to get the gradients.
